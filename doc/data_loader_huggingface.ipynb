{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44237fcd",
   "metadata": {},
   "source": [
    "## First: Load file from the Huggingface website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3866c006",
   "metadata": {},
   "source": [
    "For this we need to define a dictionary, that includes the file_name as the key and file_name.type as the value. Then we can proceed with loading the file from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5867105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_huggingface(data_files, path = \"LEAP/subsampled_low_res\"):\n",
    "    from datasets import load_dataset\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "\n",
    "    dataset = load_dataset(path, data_files=data_files)\n",
    "    \n",
    "    for f in data_files.keys():\n",
    "        data_file = f\n",
    "\n",
    "    combined_lists = []\n",
    "    for key in tqdm(dataset[data_file].features.keys()):\n",
    "        combined_lists.append(dataset[data_file][key])\n",
    "\n",
    "    hf_array = np.array(combined_lists, dtype=np.float32)\n",
    "    hf_array = np.transpose(hf_array)\n",
    "    \n",
    "    return hf_array\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4eaa44",
   "metadata": {},
   "source": [
    "## Second: Load file from the local system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45d7fb",
   "metadata": {},
   "source": [
    "For this we just need the file_name and the root_path where it is stored in your local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "061e9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climsim_utils.data_utils import *\n",
    "\n",
    "def load_original_dataset(file_name, root_path):\n",
    "\n",
    "    grid_path = root_path + 'GitHub/ClimSim/grid_info/ClimSim_low-res_grid-info.nc'\n",
    "    norm_path = root_path + 'GitHub/ClimSim/preprocessing/normalizations/'\n",
    "\n",
    "    grid_info = xr.open_dataset(grid_path)\n",
    "    input_mean = xr.open_dataset(norm_path + 'inputs/input_mean.nc')\n",
    "    input_max = xr.open_dataset(norm_path + 'inputs/input_max.nc')\n",
    "    input_min = xr.open_dataset(norm_path + 'inputs/input_min.nc')\n",
    "    output_scale = xr.open_dataset(norm_path + 'outputs/output_scale.nc')\n",
    "\n",
    "    data = data_utils(grid_info = grid_info, \n",
    "                      input_mean = input_mean, \n",
    "                      input_max = input_max, \n",
    "                      input_min = input_min, \n",
    "                      output_scale = output_scale)\n",
    "\n",
    "    data.set_to_v1_vars()\n",
    "    project_path = os.path.dirname(root_path)\n",
    "    data_path = os.path.join(project_path, 'GitHub/ClimSim/npy_files/') # files stored in this folder on local system\n",
    "    \n",
    "    final_array = data.load_npy_file(data_path + file_name)\n",
    "        \n",
    "    return final_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce4283",
   "metadata": {},
   "source": [
    "## Third: Compare to confirm that the two files are same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2bd8d0",
   "metadata": {},
   "source": [
    "Lets test using the file: scoring_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53842b03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/shreyaverma/.cache/huggingface/datasets/LEAP___parquet/LEAP--subsampled_low_res-8e54c9bc8aab12e5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb46ececcab540fd9d301b09ea8f5db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 124/124 [05:23<00:00,  2.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1681920, 124)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"scoring_input\": \"scoring_input.parquet\"} # Defining the dictionary\n",
    "hf_array = load_dataset_from_huggingface(data_files)\n",
    "hf_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521a8971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1681920, 124)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = \"/Users/shreyaverma/Documents/\"  # Giving the root path\n",
    "file_name = 'scoring_input.npy'  # Giving the file name\n",
    "final_array = load_original_dataset(file_name, root_path)\n",
    "final_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f99ea6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_array.shape == final_array.shape # Gives true, confirming we're successful in loading data from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef39f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
